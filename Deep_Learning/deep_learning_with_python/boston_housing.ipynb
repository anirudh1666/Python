{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "boston_housing.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "hj32yRkibXV1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d194a62f-4d92-405d-e40d-94bbf87263d3"
      },
      "source": [
        "# In this program we will train a network to predict the median price of \n",
        "# homes in a given Boston suburb in the mid 1970s. The dataset has different features\n",
        "# (for example crime rate) and each feature has its own scale.\n",
        "\n",
        "# This is a regression problem since we are dealing with continuous values rather \n",
        "# than discrete.\n",
        "\n",
        "import numpy as np\n",
        "from keras.datasets import boston_housing\n",
        "from keras import layers\n",
        "from keras import models\n",
        "\n",
        "(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()\n",
        "\n",
        "# First we look at the data dimensions\n",
        "print('Train data: ' + str(train_data.shape))\n",
        "print('Test data: ' + str(test_data.shape))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train data: (404, 13)\n",
            "Test data: (102, 13)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "doQcz_aec7iC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        },
        "outputId": "630cc318-2761-4622-a33a-ea0727f667c3"
      },
      "source": [
        "# As you can see there are 404 training samples and 102 test samples each \n",
        "# is a 13D vector. Each dimension in vector corresponds to a feature eg crimerate.\n",
        "\n",
        "# Train/test targets are the median values of homes in thousands of dollars.\n",
        "print('Train targets: ' + str(train_targets))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train targets: [15.2 42.3 50.  21.1 17.7 18.5 11.3 15.6 15.6 14.4 12.1 17.9 23.1 19.9\n",
            " 15.7  8.8 50.  22.5 24.1 27.5 10.9 30.8 32.9 24.  18.5 13.3 22.9 34.7\n",
            " 16.6 17.5 22.3 16.1 14.9 23.1 34.9 25.  13.9 13.1 20.4 20.  15.2 24.7\n",
            " 22.2 16.7 12.7 15.6 18.4 21.  30.1 15.1 18.7  9.6 31.5 24.8 19.1 22.\n",
            " 14.5 11.  32.  29.4 20.3 24.4 14.6 19.5 14.1 14.3 15.6 10.5  6.3 19.3\n",
            " 19.3 13.4 36.4 17.8 13.5 16.5  8.3 14.3 16.  13.4 28.6 43.5 20.2 22.\n",
            " 23.  20.7 12.5 48.5 14.6 13.4 23.7 50.  21.7 39.8 38.7 22.2 34.9 22.5\n",
            " 31.1 28.7 46.  41.7 21.  26.6 15.  24.4 13.3 21.2 11.7 21.7 19.4 50.\n",
            " 22.8 19.7 24.7 36.2 14.2 18.9 18.3 20.6 24.6 18.2  8.7 44.  10.4 13.2\n",
            " 21.2 37.  30.7 22.9 20.  19.3 31.7 32.  23.1 18.8 10.9 50.  19.6  5.\n",
            " 14.4 19.8 13.8 19.6 23.9 24.5 25.  19.9 17.2 24.6 13.5 26.6 21.4 11.9\n",
            " 22.6 19.6  8.5 23.7 23.1 22.4 20.5 23.6 18.4 35.2 23.1 27.9 20.6 23.7\n",
            " 28.  13.6 27.1 23.6 20.6 18.2 21.7 17.1  8.4 25.3 13.8 22.2 18.4 20.7\n",
            " 31.6 30.5 20.3  8.8 19.2 19.4 23.1 23.  14.8 48.8 22.6 33.4 21.1 13.6\n",
            " 32.2 13.1 23.4 18.9 23.9 11.8 23.3 22.8 19.6 16.7 13.4 22.2 20.4 21.8\n",
            " 26.4 14.9 24.1 23.8 12.3 29.1 21.  19.5 23.3 23.8 17.8 11.5 21.7 19.9\n",
            " 25.  33.4 28.5 21.4 24.3 27.5 33.1 16.2 23.3 48.3 22.9 22.8 13.1 12.7\n",
            " 22.6 15.  15.3 10.5 24.  18.5 21.7 19.5 33.2 23.2  5.  19.1 12.7 22.3\n",
            " 10.2 13.9 16.3 17.  20.1 29.9 17.2 37.3 45.4 17.8 23.2 29.  22.  18.\n",
            " 17.4 34.6 20.1 25.  15.6 24.8 28.2 21.2 21.4 23.8 31.  26.2 17.4 37.9\n",
            " 17.5 20.   8.3 23.9  8.4 13.8  7.2 11.7 17.1 21.6 50.  16.1 20.4 20.6\n",
            " 21.4 20.6 36.5  8.5 24.8 10.8 21.9 17.3 18.9 36.2 14.9 18.2 33.3 21.8\n",
            " 19.7 31.6 24.8 19.4 22.8  7.5 44.8 16.8 18.7 50.  50.  19.5 20.1 50.\n",
            " 17.2 20.8 19.3 41.3 20.4 20.5 13.8 16.5 23.9 20.6 31.5 23.3 16.8 14.\n",
            " 33.8 36.1 12.8 18.3 18.7 19.1 29.  30.1 50.  50.  22.  11.9 37.6 50.\n",
            " 22.7 20.8 23.5 27.9 50.  19.3 23.9 22.6 15.2 21.7 19.2 43.8 20.3 33.2\n",
            " 19.9 22.5 32.7 22.  17.1 19.  15.  16.1 25.1 23.7 28.7 37.2 22.6 16.4\n",
            " 25.  29.8 22.1 17.4 18.1 30.3 17.5 24.7 12.6 26.5 28.7 13.3 10.4 24.4\n",
            " 23.  20.  17.8  7.  11.8 24.4 13.8 19.4 25.2 19.4 19.4 29.1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfg70S5PdjO9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# We need to make all the features in the 13D vectors have the same scale so \n",
        "# we need to normalize it. We do this by using feature wise normalization \n",
        "# For each feature in the input data we subtract the mean of the feature and \n",
        "# divide by the standard deviation. This means the feature is centered around 0 \n",
        "# and has unit standard deviation.\n",
        "\n",
        "# Make sure to never use any quantity computed on test data. \n",
        "mean = train_data.mean(axis=0)\n",
        "train_data -= mean\n",
        "std = train_data.std(axis=0)\n",
        "train_data /= std\n",
        "\n",
        "test_data -= mean\n",
        "test_data /= std \n",
        "\n",
        "# Since we have a small sample set we use only two layers to avoid overfitting \n",
        "# In general, to avoid overfitting use smaller network\n",
        "\n",
        "# This builds the typical model for regression problem.\n",
        "def build_model():\n",
        "\n",
        "  model = models.Sequential()\n",
        "  model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],)))\n",
        "  model.add(layers.Dense(64, activation='relu'))\n",
        "  model.add(layers.Dense(1))        # No activation so it is a linear layer. This allows network to output what it wants.\n",
        "  model.compile(optimizer='rmsprop', loss='mse', metrics=['mae'])      # mse = mean squared error (square of difference between targets and prediction)\n",
        "                                                                       # mae = mean absolute error.\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O--9Fx8thJjt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "d2bfd8e5-63e7-4a74-8cd3-d034c071105e"
      },
      "source": [
        "# We could split our data into validation and training set but we don't have a lot of\n",
        "# samples which will cause a high variance since it could vary a lot \n",
        "# depending on data points we use as training and validation. Instead we use a K-Fold approach\n",
        "# which consists of splitting dataset into K partitions, making k identical models and training \n",
        "# each one on K-1 partitions while evaluating on remaining partition. The validation score \n",
        "# is the mean of the K validation scores.\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "k = 4\n",
        "num_val_samples = len(train_data) // k\n",
        "num_epochs = 100\n",
        "all_scores = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('Processing fold #', i)\n",
        "  # Get the validation set\n",
        "  val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "\n",
        "  # Prepares the training data. Basically just concats the data while missing out the \n",
        "  # validation set.\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i * num_val_samples],\n",
        "       train_data[(i + 1) * num_val_samples:]],\n",
        "       axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets[:i * num_val_samples],\n",
        "       train_targets[(i + 1) * num_val_samples:]],\n",
        "       axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs=num_epochs, batch_size=1, verbose=0)     # Verbose = 0 means train the model silently.\n",
        "  \n",
        "  val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)\n",
        "  all_scores.append(val_mae)\n",
        "\n",
        "print('All scores: ' + str(all_scores))\n",
        "print('Mean: ' + str(np.mean(all_scores)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing fold # 0\n",
            "Processing fold # 1\n",
            "Processing fold # 2\n",
            "Processing fold # 3\n",
            "All scores: [2.1424367427825928, 2.566755533218384, 2.522475242614746, 2.6809005737304688]\n",
            "Mean: 2.478142023086548\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjTjHjzkkdoc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "d92464c2-40a8-4cba-92fa-67f0311d0f8d"
      },
      "source": [
        "# As you can see the average is 2.5. This means the model is off by 2500 dollars. This isnt good enough\n",
        "# so lets retrain with 500 epochs\n",
        "\n",
        "num_epochs = 500\n",
        "all_mae_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "  print('Processing fold #', i)\n",
        "  val_data = train_data[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "  val_targets = train_targets[i * num_val_samples : (i + 1) * num_val_samples]\n",
        "\n",
        "  partial_train_data = np.concatenate(\n",
        "      [train_data[:i * num_val_samples],\n",
        "       train_data[(i + 1) * num_val_samples:]],\n",
        "       axis=0)\n",
        "  partial_train_targets = np.concatenate(\n",
        "      [train_targets[:i * num_val_samples],\n",
        "       train_targets[(i + 1) * num_val_samples:]],\n",
        "       axis=0)\n",
        "  \n",
        "  model = build_model()\n",
        "  history = model.fit(partial_train_data, partial_train_targets,\n",
        "            epochs=num_epochs, batch_size=1, verbose=0) \n",
        "  mae_history = history.history['val_mean_absolute_error']\n",
        "  all_mae_histories.append(mae_history)\n",
        "\n",
        "average_mae_history = [np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing fold # 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-0ce27e3272d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m   history = model.fit(partial_train_data, partial_train_targets,\n\u001b[1;32m     23\u001b[0m             epochs=num_epochs, batch_size=1, verbose=0) \n\u001b[0;32m---> 24\u001b[0;31m   \u001b[0mmae_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_mean_absolute_error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m   \u001b[0mall_mae_histories\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmae_history\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'val_mean_absolute_error'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAydRmcYloIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(range(1, len(average_mae_history) + 1), average_mae_history)\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Validation MAE')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}