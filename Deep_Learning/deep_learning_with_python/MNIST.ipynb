{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZwwNvqd6FaVP",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOWy-HQfFb7H",
        "colab_type": "text"
      },
      "source": [
        "This notebook contains code for training an AI on the MNIST dataset to classify handwritten digits.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nepibx3nDjOm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "223e79d8-4429-40c8-bd9e-076282062cdc"
      },
      "source": [
        "import keras \n",
        "from keras.datasets import mnist\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# Load data from MNIST dataset. There are 60000 train_images and 10000 test_images.\n",
        "# First we must prepare the input data to match our needs.\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# Reshape train_images into 2D tensor with 60000 samples and 28 * 28 features.\n",
        "train_images = train_images.reshape((60000, 28 * 28))\n",
        "\n",
        "# We turn it into float32 then divide by 255 because we want each pixel value to be\n",
        "# a float inbetween 0 - 1. The max pixel value is 255. to_cateogrical helps makes\n",
        "# the labels the right size.\n",
        "train_images = train_images.astype('float32') / 255\n",
        "train_labels = to_categorical(train_labels)\n",
        "\n",
        "test_images = test_images.reshape((10000, 28 * 28))\n",
        "test_images = test_images.astype('float32') / 255\n",
        "test_labels = to_categorical(test_labels)\n",
        "\n",
        "# We are using a linear layer structure so we use Sequential architecture\n",
        "network = models.Sequential()\n",
        "\n",
        "# Add two densely-connected layers that apply tensor operations to input data.\n",
        "# Each dense layer applies the following tensor operations relu(dot(Weight_Matrix, input) + b)\n",
        "# relu is max(x,0), dot is dot product.\n",
        "network.add(layers.Dense(512, activation='relu', input_shape=(28 * 28,)))\n",
        "\n",
        "# Returns probability scores for each digit. They sum to 1.\n",
        "network.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# categorical_crossentroy is loss function. rmsprop specifies to use mini\n",
        "# batch stochastical gradient based descent. \n",
        "network.compile(optimizer='rmsprop',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "# This is the trianing loop. The network iterates over batches of 128 samples\n",
        "# 5 times over.\n",
        "network.fit(train_images, train_labels, epochs=5, batch_size=128)\n",
        "\n",
        "# Now we run our AI on the test images to see how it performs\n",
        "test_loss, test_acc = network.evaluate(test_images, test_labels)\n",
        "print('test_acc:', test_acc)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 2s 33us/step - loss: 0.2544 - accuracy: 0.9254\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.1038 - accuracy: 0.9689\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 2s 32us/step - loss: 0.0689 - accuracy: 0.9797\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0497 - accuracy: 0.9846\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 2s 31us/step - loss: 0.0380 - accuracy: 0.9888\n",
            "10000/10000 [==============================] - 1s 74us/step\n",
            "test_acc: 0.9805999994277954\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}